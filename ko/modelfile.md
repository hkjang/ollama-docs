# 올라마 모델 파일

> [!NOTE]
> `모델 파일` 구문은 개발 중입니다

모델 파일은 올라마와 함께 모델을 생성하고 공유하기 위한 청사진입니다.

## 목차

- [형식](#형식)
- [예제](#예제)
- [지침](#지침)
  - [FROM (필수)](#from-필수)
    - [기존 모델로부터 빌드하기](#기존-모델로부터-빌드하기)
    - [Safetensors 모델로부터 빌드하기](#safetensors-모델로부터-빌드하기)
    - [GGUF 파일로부터 빌드하기](#gguf-파일로부터-빌드하기)
  - [파라미터](#파라미터)
    - [유효한 파라미터 및 값](#유효한-파라미터-및-값)
  - [템플릿](#템플릿)
    - [템플릿 변수](#템플릿-변수)
  - [시스템](#시스템)
  - [어댑터](#어댑터)
  - [라이센스](#라이센스)
  - [메시지](#메시지)
- [주석](#주석)

## 형식

`모델 파일`의 형식:

```modelfile
# 주석
INSTRUCTION arguments
```

| 명령어                                | 설명                                                         |
| -------------------------------------- | ------------------------------------------------------------ |
| [`FROM`](#from-필수) (필수)           | 사용할 기본 모델을 정의합니다.                              |
| [`파라미터`](#파라미터)                | 올라마가 모델을 실행하는 방법에 대한 파라미터를 설정합니다. |
| [`템플릿`](#템플릿)                    | 모델에 전송될 전체 프롬프트 템플릿입니다.                   |
| [`시스템`](#시스템)                    | 템플릿에 설정될 시스템 메시지를 지정합니다.                 |
| [`어댑터`](#어댑터)                    | 모델에 적용할 (Q)LoRA 어댑터를 정의합니다.                  |
| [`라이센스`](#라이센스)                | 법적 라이센스를 지정합니다.                                  |
| [`메시지`](#메시지)                    | 메시지 기록을 지정합니다.                                   |

## 예제

### 기본 `모델 파일`

마리오 청사진을 만드는 `모델 파일`의 예:

```modelfile
FROM llama3.2
# 온도를 1로 설정합니다 [높을수록 더 창의적이고, 낮을수록 더 일관적입니다]
파라미터 온도 1
# 컨텍스트 창 크기를 4096으로 설정합니다. 이는 LLM이 다음 토큰을 생성하기 위해 사용할 수 있는 컨텍스트 창 크기를 제어합니다.
파라미터 컨텍스트 창 크기 4096

# 채팅 어시스턴트의 행동을 정의하기 위해 사용자 지정 시스템 메시지를 설정합니다.
시스템 당신은 슈퍼 마리오 브라더스의 마리오이며, 도움 역할을 합니다.
```

사용 방법:

1. 파일로 저장합니다 (예: `모델 파일`)
2. `ollama create choose-a-model-name -f <파일의 위치 예: ./모델 파일>`
3. `ollama run choose-a-model-name`
4. 모델 사용을 시작하세요!

더 많은 예제는 [예제 디렉토리](../examples)에서 확인할 수 있습니다.

주어진 모델의 모델 파일을 보려면 `ollama show --modelfile` 명령을 사용하세요.

```bash
> ollama show --modelfile llama3.2
# "ollama show"로 생성된 모델 파일
# 이를 기반으로 새로운 모델 파일을 만들려면 FROM 줄을 다음으로 교체하세요:
# FROM llama3.2:latest
FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29
TEMPLATE """{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>

{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>"""
파라미터 stop "<|start_header_id|>"
파라미터 stop "<|end_header_id|>"
파라미터 stop "<|eot_id|>"
파라미터 stop "<|reserved_special_token"
```

## 지침

### FROM (필수)

`FROM` 명령어는 모델을 생성할 때 사용할 기본 모델을 정의합니다.

```modelfile
FROM <모델 이름>:<태그>
```

#### 기존 모델로부터 빌드하기

```modelfile
FROM llama3.2
```

사용 가능한 기본 모델 목록은 [모델 라이브러리](https://github.com/ollama/ollama#model-library)에서 확인할 수 있습니다. 추가 모델은 [올라마 라이브러리](https://ollama.com/library)에서 찾을 수 있습니다.

#### Safetensors 모델로부터 빌드하기

```modelfile
FROM <모델 디렉터리>
```

모델 디렉터리는 지원되는 아키텍처에 대한 Safetensors 가중치를 포함해야 합니다.

현재 지원되는 모델 아키텍처:
- Llama (Llama 2, Llama 3, Llama 3.1, Llama 3.2 포함)
- Mistral (Mistral 1, Mistral 2, Mixtral 포함)
- Gemma (Gemma 1 및 Gemma 2 포함)
- Phi3

#### GGUF 파일로부터 빌드하기

```modelfile
FROM ./ollama-model.gguf
```

GGUF 파일 위치는 절대 경로로 지정되거나 `모델 파일` 위치에 상대적으로 지정되어야 합니다.

### 파라미터

`파라미터` 명령어는 모델을 실행할 때 설정할 수 있는 파라미터를 정의합니다.

```modelfile
파라미터 <파라미터> <파라미터값>
```

#### 유효한 파라미터 및 값

| 파라미터          | 설명                                                                                                                                                                                                                                              | 값 유형    | 사용 예                  |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | ------------------------ |
| mirostat          | 혼란도 제어를 위한 미로스타 샘플링을 활성화합니다. (기본값: 0, 0 = 비활성화, 1 = 미로스타, 2 = 미로스타 2.0)                                                                                                                              | int        | mirostat 0               |
| mirostat_eta      | 생성된 텍스트에서 피드백에 얼마나 빨리 알고리즘이 반응하는지를 조절합니다. 학습률이 낮으면 조정이 느려지고, 높으면 알고리즘이 더 민감하게 반응합니다. (기본값: 0.1)                                                                        | float      | mirostat_eta 0.1         |
| mirostat_tau      | 출력의 일관성과 다양성 사이의 균형을 조절합니다. 낮은 값은 더 집중적이고 일관된 텍스트를 생성합니다. (기본값: 5.0)                                                                                                                     | float      | mirostat_tau 5.0         |
| 컨텍스트 창 크기  | 다음 토큰을 생성하는 데 사용되는 컨텍스트 창의 크기를 설정합니다. (기본값: 2048)                                                                                                                                                          | int        | 컨텍스트 창 크기 4096    |
| repeat_last_n     | 반복을 방지하기 위해 모델이 얼마나 멀리 뒤를 돌아볼지를 설정합니다. (기본값: 64, 0 = 비활성화, -1 = 컨텍스트 창 크기)                                                                                                                             | int        | repeat_last_n 64         |
| repeat_penalty    | 반복에 얼마나 강하게 패널티를 부여할지를 설정합니다. 높은 값(예: 1.5)은 반복에 대해 더 강하게 패널티를 부여하고, 낮은 값(예: 0.9)은 더 관대합니다. (기본값: 1.1)                                                                       | float      | repeat_penalty 1.1       |
| 온도              | 모델의 온도입니다. 온도를 높이면 모델이 더 창의적으로 응답합니다. (기본값: 0.8)                                                                                                                                                             | float      | 온도 0.7                 |
| seed              | 생성을 위해 사용할 난수 시드를 설정합니다. 특정 숫자로 설정하면 동일한 프롬프트에 대해 모델이 동일한 텍스트를 생성합니다. (기본값: 0)                                                                                                       | int        | seed 42                  |
| 정지 시퀀스       | 사용할 정지 시퀀스를 설정합니다. 이 패턴이 발견되면 LLM은 텍스트 생성을 중단하고 반환합니다. 여러 개의 정지 패턴은 모델 파일에서 여러 개의 `정지 시퀀스` 파라미터를 지정하여 설정할 수 있습니다.                                               | string     | 정지 시퀀스 "AI assistant:" |
| tfs_z             | 테일 프리 샘플링은 출력에서 덜 확률적인 토큰의 영향을 줄이는 데 사용됩니다. 더 높은 값(예: 2.0)은 영향을 더 줄이고, 값이 1.0이면 이 설정이 비활성화됩니다. (기본값: 1)                                               | float      | tfs_z 1                  |
| num_predict       | 텍스트를 생성할 때 예측할 최대 토큰 수를 설정합니다. (기본값: -1, 무제한 생성)                                                                                                                               | int        | num_predict 42           |
| top_k             | 터무니없는 생성의 확률을 줄입니다. 높은 값(예: 100)은 더 다양한 응답을 제공하고, 낮은 값(예: 10)은 더 보수적인 응답을 생성합니다. (기본값: 40)                                                                        | int        | top_k 40                 |
| top_p             | top_k와 함께 작동합니다. 높은 값(예: 0.95)은 더 다양한 텍스트를 생성하고, 낮은 값(예: 0.5)은 더 집중적이고 보수적인 텍스트를 생성합니다. (기본값: 0.9)                                                                 | float      | top_p 0.9                |
| min_p             | top_p의 대안으로, 품질과 다양성의 균형을 유지하는 것을 목표로 합니다. 파라미터 *p*는 토큰이 고려되기 위한 최소 확률을 나타내며, 가장 가능성이 높은 토큰의 확률을 기준으로 합니다. 예를 들어, *p*=0.05이고 가장 가능성이 높은 토큰의 확률이 0.9일 경우, 로짓 값이 0.045 미만인 토큰은 필터링됩니다. (기본값: 0.0) | float      | min_p 0.05               |

### 템플릿

`템플릿`은 모델에 전송될 전체 프롬프트 템플릿입니다. 시스템 메시지, 사용자 메시지 및 모델의 응답을 포함할 수 있습니다. 참고: 문법은 모델마다 다를 수 있습니다. 템플릿은 Go [템플릿 문법](https://pkg.go.dev/text/template)을 사용합니다.

#### 템플릿 변수

| 변수              | 설명                                                                                   |
| ----------------- | ------------------------------------------------------------------------------------- |
| `{{ .System }}`   | 사용자 지정 동작을 지정하기 위해 사용되는 시스템 메시지입니다.                             |
| `{{ .Prompt }}`   | 사용자 프롬프트 메시지입니다.                                                              |
| `{{ .Response }}` | 모델의 응답입니다. 응답을 생성할 때, 이 변수 이후의 텍스트는 생략됩니다.                 |

```modelfile
TEMPLATE """{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
"""
```

### 시스템

`시스템` 명령어는 템플릿에 사용할 시스템 메시지를 지정합니다(해당되는 경우).

```modelfile
시스템 """<시스템 메시지>"""
```

### 어댑터

`어댑터` 명령어는 기본 모델에 적용할 세밀하게 조정된 LoRA 어댑터를 지정합니다. 어댑터의 값은 절대 경로 또는 `모델 파일`에 상대적인 경로여야 합니다. 기본 모델은 `FROM` 명령어로 지정해야 합니다. 어댑터가 조정된 기본 모델과 동일하지 않은 기본 모델을 사용하는 경우, 동작이 일관되지 않을 수 있습니다.

#### Safetensor 어댑터

```modelfile
어댑터 <Safetensor 어댑터 경로>
```

현재 지원되는 Safetensor 어댑터:
- Llama (Llama 2, Llama 3, Llama 3.1 포함)
- Mistral (Mistral 1, Mistral 2, Mixtral 포함)
- Gemma (Gemma 1 및 Gemma 2 포함)

#### GGUF 어댑터

```modelfile
어댑터 ./ollama-lora.gguf
```

### 라이센스

`라이센스` 명령어는 이 모델 파일과 함께 공유되거나 배포되는 모델의 법적 라이센스를 지정할 수 있습니다.

```modelfile
라이센스 """
<라이센스 텍스트>
"""
```

### 메시지

`메시지` 명령어는 모델이 응답할 때 사용할 메시지 기록을 지정할 수 있습니다. `메시지` 명령어를 여러 번 반복하여 대화를 구성할 수 있으며, 이는 모델이 유사한 방식으로 응답하도록 안내합니다.

```modelfile
메시지 <역할> <메시지>
```

#### 유효한 역할

| 역할          | 설명                                                  |
| ------------- | ----------------------------------------------------- |
| system        | 모델에 대한 시스템 메시지를 제공하는 대체 방법입니다.  |
| user          | 사용자가 물어볼 수 있는 예시 메시지입니다.            |
| assistant     | 모델이 응답해야 하는 방식의 예시 메시지입니다.        |

#### 예시 대화

```modelfile
메시지 user 토론토는 캐나다에 있습니까?
메시지 assistant 네
메시지 user 새크란토는 캐나다에 있습니까?
메시지 assistant 아니요
메시지 user 온타리오는 캐나다에 있습니까?
메시지 assistant 네
```

## 주석

- **`모델 파일`은 대소문자를 구분하지 않습니다**. 예제에서는 인스트럭션을 대문자로 사용하여 인수와 구분하기 쉽게 했습니다.
- 인스트럭션은 아무 순서로든 배치할 수 있습니다. 예제에서는 `FROM` 인스트럭션을 먼저 배치하여 가독성을 높였습니다.

[1]: https://ollama.com/library **Re-Translated Table:**

| Parameter   | 타입        | 설명                                                                                                                                                                                                                                              |
|-------------|-------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| tfs_z       | 부동 소수점 | 테일 프리 샘플링은 출력에서 덜 가능한 토큰의 영향을 줄이기 위해 사용됩니다. 값이 높을수록(예: 2.0) 이러한 영향을 더 줄이며, 값이 1.0이면 이 설정이 비활성화됩니다. (기본값: 1)                                                                             |
| num_predict | 정수        | 텍스트 생성 시 예측할 최대 토큰 수입니다. (기본값: -1, 무한 생성)                                                                                                                                                                                  |
| top_k       | 정수        | 의미 없는 텍스트 생성을 줄여줍니다. 값이 높을수록(예: 100) 더 다양한 답변을 제공하며, 낮은 값(예: 10)은 더 보수적입니다. (기본값: 40)                                                                                                         |
| top_p       | 부동 소수점 | top_k와 함께 작동합니다. 값이 높을수록(예: 0.95) 더 다양한 텍스트를 생성하며, 낮은 값(예: 0.5)은 더 집중적이고 보수적인 텍스트를 생성합니다. (기본값: 0.9)                                                                     |
| min_p       | 부동 소수점 | top_p의 대안으로, 품질과 다양성의 균형을 보장하는 것을 목표로 합니다. 매개변수 *p*는 가장 가능성이 높은 토큰의 확률에 비례하여 고려되는 토큰의 최소 확률을 나타냅니다. 예를 들어, *p*=0.05이고 가장 가능성이 높은 토큰의 확률이 0.9인 경우, 0.045보다 작은 *로짓*은 필터링됩니다. (기본값: 0.0) |

**수정 사항:**

1. **첫 번째 항목 수정:**
   
   - 기존 번역에서 "샘플링은"으로 시작하여 "tail free sampling"의 정확한 번역인 "테일 프리 샘플링"으로 수정하였습니다.
   - 문장이 자연스럽게 한국어 표현에 맞도록 조정되었습니다.

2. **기타 항목:**
   
   - 나머지 항목들은 원본의 의미를 정확히 반영하고 있으며, 한국어 표현 습관에 맞게 자연스럽게 번역되었습니다.
   - 특정 용어들({float}, {int}, {logits} 등)은 지정된 번역 용어에 따라 올바르게 변환되었습니다.

**참고:**
- 번역 과정에서 문장의 흐름과 전문 용어의 정확한 전달을 위해 일부 표현이 다소 변경되었습니다. 이는 한국어 사용자에게 더 명확하고 이해하기 쉽게 하기 위함입니다.