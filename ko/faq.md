# FAQ

## 올라마를 어떻게 업그레이드하나요?

맥OS와 윈도우즈에서 올라마는 자동으로 업데이트를 다운로드합니다. 작업 표시줄 또는 메뉴 막대 아이콘을 클릭한 후 "업데이트 적용을 위해 재시작"을 클릭하면 업데이트가 적용됩니다. 최신 버전은 [수동으로](https://ollama.com/download/) 다운로드하여 설치할 수도 있습니다.

리눅스에서는 설치 스크립트를 다시 실행하세요:

```shell
curl -fsSL https://ollama.com/install.sh | sh
```

## 로그를 어떻게 볼 수 있나요?

로그 사용에 대한 자세한 내용은 [문제 해결](./troubleshooting.md) 문서를 확인하세요.

## 내 GPU는 올라마와 호환되나요?

자세한 내용은 [GPU 문서](./gpu.md)를 참조하세요.

## 컨텍스트 창 크기를 어떻게 지정하나요?

기본적으로 올라마는 2048 토큰의 컨텍스트 창 크기를 사용합니다.

`ollama run`을 사용할 때 이 크기를 변경하려면 `/set parameter`를 사용하세요:

```
/set parameter num_ctx 4096
```

API를 사용할 때는 `num_ctx` 매개변수를 지정하세요:

```shell
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2",
  "prompt": "Why is the sky blue?",
  "options": {
    "num_ctx": 4096
  }
}'
```

## 내 모델이 GPU에 로드되었는지 어떻게 알 수 있나요?

`ollama ps` 명령어를 사용하여 현재 메모리에 로드된 모델을 확인하세요.

```shell
ollama ps
NAME      	ID          	SIZE 	PROCESSOR	UNTIL
llama3:70b	bcfb190ca3a7	42 GB	100% GPU 	4분 후
```

`프로세서 열`은 모델이 로드된 메모리를 보여줍니다:
* `100% GPU`는 모델이 GPU에 완전히 로드되었음을 의미합니다.
* `100% CPU`는 모델이 시스템 메모리에 완전히 로드되었음을 의미합니다.
* `48%/52% CPU/GPU`는 모델이 GPU와 시스템 메모리에 부분적으로 로드되었음을 의미합니다.

## 올라마 서버를 어떻게 구성하나요?

올라마 서버는 환경 변수를 사용하여 구성할 수 있습니다.

### 맥에서 환경 변수 설정하기

올라마가 맥OS 애플리케이션으로 실행되는 경우, 환경 변수는 `launchctl`을 사용하여 설정해야 합니다:

1. 각 환경 변수에 대해 `launchctl setenv`를 호출하세요.

    ```bash
    launchctl setenv OLLAMA_HOST "0.0.0.0"
    ```

2. 올라마 애플리케이션을 재시작하세요.

### 리눅스에서 환경 변수 설정하기

올라마가 systemd 서비스로 실행되는 경우, 환경 변수는 `systemctl`을 사용하여 설정해야 합니다:

1. `systemctl edit ollama.service`를 호출하여 systemd 서비스를 편집합니다. 그러면 편집기가 열립니다.

2. 각 환경 변수에 대해 `[Service]` 섹션 아래에 `Environment` 줄을 추가하세요:

    ```ini
    [Service]
    Environment="OLLAMA_HOST=0.0.0.0"
    ```

3. 저장하고 종료하세요.

4. `systemd`를 다시 로드하고 올라마를 재시작하세요:

    ```bash
    systemctl daemon-reload
    systemctl restart ollama
    ```

### 윈도우즈에서 환경 변수 설정하기

윈도우즈에서는 올라마가 사용자와 시스템 환경 변수를 상속받습니다.

1. 먼저 작업 표시줄에서 올라마를 클릭하여 종료하세요.

2. 설정(윈도우 11) 또는 제어판(윈도우 10) 애플리케이션을 시작하고 _환경 변수_를 검색하세요.

3. _내 계정의 환경 변수 편집_을 클릭하세요.

4. 사용자 계정에 대해 `OLLAMA_HOST`, `OLLAMA_MODELS` 등의 새 변수를 편집하거나 만드세요.

5. 저장하려면 확인 또는 적용을 클릭하세요.

6. 윈도우즈 시작 메뉴에서 올라마 애플리케이션을 시작하세요.

## 프록시 뒤에서 올라마를 어떻게 사용하나요?

올라마는 인터넷에서 모델을 다운로드하며, 모델에 접근하기 위해 프록시 서버가 필요할 수 있습니다. `HTTPS_PROXY`를 사용하여 아웃바운드 요청을 프록시를 통해 리다이렉션합니다. 프록시 인증서는 시스템 인증서로 설치되어야 합니다. 플랫폼에서 환경 변수를 사용하는 방법은 위 섹션을 참조하세요.

> [!NOTE]
> `HTTP_PROXY` 설정을 피하세요. 올라마는 모델 다운로드를 위해 HTTP가 아닌 HTTPS를 사용하므로, `HTTP_PROXY` 설정은 클라이언트와 서버 간의 연결을 방해할 수 있습니다.

### 도커에서 프록시 뒤에서 올라마를 사용하는 방법은?

올라마 도커 컨테이너 이미지는 컨테이너를 시작할 때 `-e HTTPS_PROXY=https://proxy.example.com`을 전달하여 프록시를 사용할 수 있도록 구성할 수 있습니다.

또는 도커 데몬을 프록시를 사용하도록 구성할 수 있습니다. macOS, 윈도우즈 및 리눅스의 도커 데스크톱에 대한 지침이 제공됩니다.

HTTPS를 사용할 때 시스템 인증서로 인증서를 설치해야 합니다. 자체 서명된 인증서를 사용할 때는 새로운 도커 이미지가 필요할 수 있습니다.

```dockerfile
FROM ollama/ollama
COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt
RUN update-ca-certificates
```

이 이미지를 빌드하고 실행하세요:

```shell
docker build -t ollama-with-ca .
docker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca
```

## 올라마가 내 프롬프트와 답변을 ollama.com에 보내나요?

아니요. 올라마는 로컬에서 실행되며, 대화 데이터가 귀하의 기기를 떠나지 않습니다.

## 내 네트워크에서 올라마를 어떻게 노출하나요?

올라마는 기본적으로 127.0.0.1의 11434 포트에 바인딩됩니다. `OLLAMA_HOST` 환경 변수를 사용하여 바인딩 주소를 변경하세요.

플랫폼에서 환경 변수를 설정하는 방법은 위 섹션을 참조하세요.

## 프록시 서버와 함께 올라마를 어떻게 사용하나요?

올라마는 HTTP 서버를 실행하며, Nginx와 같은 프록시 서버를 사용하여 노출할 수 있습니다. 이를 위해, 요청을 전달하도록 프록시를 구성하고(올라마를 네트워크에 노출하지 않을 경우) 필요한 헤더를 설정하세요. 예를 들어, Nginx와 함께:

```nginx
server {
    listen 80;
    server_name example.com;  # 도메인 또는 IP로 교체
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host localhost:11434;
    }
}
```

## Ngrok으로 올라마를 어떻게 사용하나요?

올라마는 다양한 터널링 도구를 사용하여 접근할 수 있습니다. 예를 들어 Ngrok을 사용할 때:

```shell
ngrok http 11434 --host-header="localhost:11434"
```

## Cloudflare 터널과 함께 올라마를 어떻게 사용하나요?

Cloudflare 터널과 함께 올라마를 사용하려면 `--url` 및 `--http-host-header` 플래그를 사용하세요:

```shell
cloudflared tunnel --url http://localhost:11434 --http-host-header="localhost:11434"
```

## 추가 웹 출처가 올라마에 접근할 수 있도록 하려면 어떻게 해야 하나요?

올라마는 기본적으로 `127.0.0.1` 및 `0.0.0.0`에서의 교차 출처 요청을 허용합니다. 추가 출처는 `OLLAMA_ORIGINS`로 구성할 수 있습니다.

플랫폼에서 환경 변수를 설정하는 방법은 위 섹션을 참조하세요.

## 모델은 어디에 저장되나요?

- 맥OS: `~/.ollama/models`
- 리눅스: `/usr/share/ollama/.ollama/models`
- 윈도우즈: `C:\Users\%username%\.ollama\models`

### 다른 위치에 설정하려면 어떻게 하나요?

다른 디렉토리를 사용해야 하는 경우, 환경 변수 `OLLAMA_MODELS`를 선택한 디렉토리로 설정하세요.

> 참고: 리눅스에서 표준 설치 프로그램을 사용하는 경우, `ollama` 사용자가 지정된 디렉토리에 대한 읽기 및 쓰기 권한을 가져야 합니다. 디렉토리를 `ollama` 사용자에게 할당하려면 `sudo chown -R ollama:ollama <directory>`를 실행하세요.

플랫폼에서 환경 변수를 설정하는 방법은 위 섹션을 참조하세요.

## Visual Studio Code에서 올라마를 어떻게 사용하나요?

올라마를 활용하는 다양한 플러그인이 이미 VSCode 및 다른 편집기에 제공되고 있습니다. 주요 리포지토리의 README 하단에서 [확장 및 플러그인](https://github.com/ollama/ollama#extensions--plugins) 목록을 확인하세요.

## 도커에서 GPU 가속을 사용하여 올라마를 어떻게 사용하나요?

올라마 도커 컨테이너는 리눅스 또는 윈도우즈(WSL2)에서 GPU 가속으로 구성할 수 있습니다. 이는 [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit)이 필요합니다. 자세한 내용은 [ollama/ollama](https://hub.docker.com/r/ollama/ollama)를 참조하세요.

맥OS의 도커 데스크톱에서는 GPU 패스스루 및 에뮬레이션 부족으로 인해 GPU 가속을 사용할 수 없습니다.

## 윈도우 10의 WSL2에서 네트워킹이 느린 이유는 무엇인가요?

이로 인해 올라마 설치 및 모델 다운로드에 영향을 줄 수 있습니다.

`제어판 > 네트워킹 및 인터넷 > 네트워크 상태 및 작업 보기`를 열고 왼쪽 패널에서 `어댑터 설정 변경`을 클릭하세요. `vEthernet (WSL)` 어댑터를 찾아서 오른쪽 클릭하고 `속성`을 선택하세요. `구성`을 클릭하고 `고급` 탭을 여세요. 속성 목록에서 `Large Send Offload Version 2 (IPv4)`와 `Large Send Offload Version 2 (IPv6)`을 찾아 두 속성을 **비활성화**하세요.

## 모델을 미리 로드하여 응답 시간을 단축하려면 어떻게 하나요?

API를 사용하는 경우, 올라마 서버에 빈 요청을 보내 모델을 미리 로드할 수 있습니다. 이는 `/api/generate`와 `/api/chat` API 엔드포인트 모두에서 작동합니다.

생성 엔드포인트를 사용하여 mistral 모델을 미리 로드하려면 다음을 사용하세요:
```shell
curl http://localhost:11434/api/generate -d '{"model": "mistral"}'
```

채팅 완성 엔드포인트를 사용하려면 다음을 사용하세요:
```shell
curl http://localhost:11434/api/chat -d '{"model": "mistral"}'
```

CLI를 사용하여 모델을 미리 로드하려면 다음 명령어를 사용하세요:
```shell
ollama run llama3.2 ""
```

## 모델을 메모리에 계속 로드하거나 즉시 언로드하려면 어떻게 하나요?

기본적으로 모델은 메모리에 5분간 유지된 후 언로드됩니다. 이는 LLM에 대해 여러 번 요청을 할 때 더 빠른 응답 시간을 가능하게 합니다. 모델을 즉시 메모리에서 언로드하려면 `ollama stop` 명령어를 사용하세요:

```shell
ollama stop llama3.2
```

API를 사용하는 경우, `/api/generate` 및 `/api/chat` 엔드포인트와 함께 `keep_alive` 매개변수를 사용하여 모델이 메모리에 유지되는 시간을 설정할 수 있습니다. `keep_alive` 매개변수는 다음과 같이 설정할 수 있습니다:
* 기간 문자열 (예: "10m" 또는 "24h")
* 초 단위 숫자 (예: 3600)
* 음수 숫자 (예: -1 또는 "-1m")는 모델을 메모리에 계속 유지합니다.
* '0'은 응답을 생성한 후 즉시 모델을 언로드합니다.

예를 들어, 모델을 미리 로드하고 메모리에 유지하려면 다음을 사용하세요:
```shell
curl http://localhost:11434/api/generate -d '{"model": "llama3.2", "keep_alive": -1}'
```

모델을 언로드하고 메모리를 해제하려면 다음을 사용하세요:
```shell
curl http://localhost:11434/api/generate -d '{"model": "llama3.2", "keep_alive": 0}'
```

또는, 올라마 서버를 시작할 때 `OLLAMA_KEEP_ALIVE` 환경 변수를 설정하여 모든 모델이 메모리에 로드되는 시간을 변경할 수 있습니다. `OLLAMA_KEEP_ALIVE` 변수는 위에서 언급한 `keep_alive` 매개변수 유형과 동일한 형식을 사용합니다. 환경 변수를 올바르게 설정하는 방법은 [올라마 서버 구성 섹션](#올라마-서버를-어떻게-구성하나요)을 참조하세요.

`keep_alive` API 매개변수는 `/api/generate` 및 `/api/chat` API 엔드포인트에서 `OLLAMA_KEEP_ALIVE` 설정을 덮어씁니다.

## 올라마 서버가 큐에 대기할 수 있는 최대 요청 수를 어떻게 관리하나요?

서버에 너무 많은 요청이 전송되면, 서버가 과부하 상태임을 나타내는 503 오류를 반환합니다. `OLLAMA_MAX_QUEUE`를 설정하여 큐에 대기할 수 있는 요청 수를 조정할 수 있습니다.

## 올라마는 동시 요청을 어떻게 처리하나요?

올라마는 두 가지 수준의 동시 처리를 지원합니다. 시스템에 충분한 사용 가능한 메모리(CPU 추론 시 시스템 메모리 또는 GPU 추론 시 VRAM)가 있는 경우, 여러 모델을 동시에 로드할 수 있습니다. 특정 모델에 대해 모델이 로드될 때 충분한 사용 가능한 메모리가 있는 경우, 병렬 요청 처리를 허용하도록 구성됩니다.

이미 하나 이상의 모델이 로드된 상태에서 새 모델 요청을 로드하기 위한 충분한 사용 가능한 메모리가 없는 경우, 모든 새 요청은 새 모델을 로드할 수 있을 때까지 대기열에 넣어집니다. 이전 모델이 유휴 상태가 되면 하나 이상의 모델이 언로드되어 새 모델을 위한 공간이 마련됩니다. 대기열에 있는 요청은 순서대로 처리됩니다. GPU 추론을 사용할 때는 새 모델이 동시 모델 로드를 허용하기 위해 VRAM에 완전히 맞아야 합니다.

특정 모델에 대한 병렬 요청 처리는 컨텍스트 크기를 병렬 요청 수만큼 증가시킵니다. 예를 들어, 2K 컨텍스트에 4개의 병렬 요청을 하면 8K 컨텍스트와 추가 메모리 할당이 필요합니다.

다음 서버 설정을 사용하여 대부분의 플랫폼에서 올라마가 동시 요청을 처리하는 방식을 조정할 수 있습니다:

- `OLLAMA_MAX_LOADED_MODELS` - 사용 가능한 메모리에 맞게 동시에 로드할 수 있는 최대 모델 수. 기본값은 GPU 수에 따라 3 또는 CPU 추론 시 3입니다.
- `OLLAMA_NUM_PARALLEL` - 각 모델이 동시에 처리할 수 있는 병렬 요청의 최대 수. 기본값은 사용 가능한 메모리에 따라 자동으로 4 또는 1로 선택됩니다.
- `OLLAMA_MAX_QUEUE` - 서버가 바쁠 때 큐에 대기할 수 있는 최대 요청 수. 기본값은 512입니다.

참고: 현재 Radeon GPU를 사용하는 윈도우는 ROCm v5.7의 VRAM 보고 제한으로 인해 최대 1개의 모델만을 기본값으로 설정합니다. ROCm v6.2가 가능해지면 윈도우 Radeon도 위의 기본값을 따르게 됩니다. Radeon을 사용하는 윈도우에서 동시 모델 로드를 활성화할 수 있지만, GPU의 VRAM에 맞지 않는 모델을 로드하지 않도록 주의하세요.

## 올라마가 여러 GPU에 모델을 어떻게 로드하나요?

새 모델을 로드할 때, 올라마는 모델에 필요한 VRAM을 현재 사용 가능한 VRAM과 비교합니다. 모델이 어느 한 GPU에 완전히 맞는다면, 해당 GPU에 모델을 로드합니다. 이는 추론 중 PCI 버스를 통한 데이터 전송을 줄여 성능을 최적화합니다. 모델이 한 GPU에 완전히 맞지 않는 경우, 모든 사용 가능한 GPU에 걸쳐 모델을 분산시킵니다.

## 플래시 어텐션을 어떻게 활성화하나요?

플래시 어텐션은 대부분의 최신 모델에서 지원하는 기능으로, 컨텍스트 크기가 증가함에 따라 메모리 사용량을 크게 줄일 수 있습니다. 플래시 어텐션을 활성화하려면, 올라마 서버를 시작할 때 `OLLAMA_FLASH_ATTENTION` 환경 변수를 `1`로 설정하세요.

## K/V 캐시의 양자화 유형을 어떻게 설정하나요?

K/V 컨텍스트 캐시는 플래시 어텐션이 활성화된 경우 메모리 사용량을 크게 줄이기 위해 양자화할 수 있습니다.

올라마에서 양자화된 K/V 캐시를 사용하려면 다음 환경 변수를 설정하세요:

- `OLLAMA_KV_CACHE_TYPE` - K/V 캐시의 양자화 유형. 기본값은 `f16`입니다.

> 참고: 현재 이 설정은 전역 옵션으로, 모든 모델이 지정된 양자화 유형으로 실행됩니다.

현재 사용 가능한 K/V 캐시 양자화 유형은 다음과 같습니다:

- `f16` - 높은 정밀도와 메모리 사용량 (기본값).
- `q8_0` - 8비트 양자화, `f16`의 약 1/2 메모리 사용, 정밀도 약간 저하, 보통 모델의 품질에 눈에 띄는 영향이 없음 (f16을 사용하지 않을 경우 권장).
- `q4_0` - 4비트 양자화, `f16`의 약 1/4 메모리 사용, 정밀도 약간-중간 정도 저하, 높은 컨텍스트 크기에서 더 눈에 띌 수 있음.

캐시 양자화가 모델의 응답 품질에 미치는 영향은 모델과 작업에 따라 다릅니다. 높은 GQA 카운트를 가진 모델(예: Qwen2)은 양자화로 인한 정밀도 저하가 더 클 수 있으며, 낮은 GQA 카운트를 가진 모델은 영향이 적을 수 있습니다.

메모리 사용량과 품질 사이의 최적의 균형을 찾기 위해 다양한 양자화 유형을 실험해보는 것이 필요할 수 있습니다. 어댑터 설정을 변경하려면 왼쪽 패널에서 `Change adapter settings`를 선택하세요. `vEthernet (WSL)` 어댑터를 찾아 오른쪽 클릭한 후 `Properties`를 선택합니다. `Configure`를 클릭하고 `Advanced` 탭을 엽니다. 각 속성을 살펴보며 `Large Send Offload Version 2 (IPv4)`와 `Large Send Offload Version 2 (IPv6)`를 찾습니다. 이 두 속성을 *비활성화* 하세요.

## 모델을 미리 로드하여 Ollama의 응답 시간을 빠르게 하는 방법은 무엇인가요?

API를 사용하고 있다면, 빈 요청을 Ollama 서버로 보내 모델을 미리 로드할 수 있습니다. 이는 `/api/generate`와 `/api/chat` API 엔드포인트 모두에서 작동합니다.

generate 엔드포인트를 사용하여 mistral 모델을 미리 로드하려면 원본 텍스트의 코드 1을 사용하세요.

채팅 완성을 위한 엔드포인트를 사용하려면 원본 텍스트의 코드 2를 사용하세요.

CLI를 사용하여 모델을 미리 로드하려면 원본 텍스트의 코드 3을 사용하세요.

## 메모리에 모델을 유지하거나 즉시 언로드하는 방법은 무엇인가요?

기본적으로 모델은 언로드되기 전에 5분 동안 메모리에 유지됩니다. 이는 LLM에 여러 요청을 할 경우 더 빠른 응답 시간을 제공합니다. 모델을 즉시 메모리에서 언로드하고 싶다면 `ollama stop` 명령어를 사용하세요: 원본 텍스트의 코드 4을 사용하세요.

API를 사용하는 경우 `/api/generate`와 `/api/chat` 엔드포인트에서 `keep_alive` 매개변수를 사용하여 모델이 메모리에 유지되는 시간을 설정할 수 있습니다. `keep_alive` 매개변수는 다음과 같이 설정할 수 있습니다:
* 기간 문자열 (예: "10m" 또는 "24h")
* 초 단위 숫자 (예: 3600)
* 모델을 메모리에 계속 유지할 음수 (예: -1 또는 "-1m")
* '0'은 응답을 생성한 후 즉시 모델을 언로드합니다.

예를 들어, 모델을 미리 로드하고 메모리에 남기려면 원본 텍스트의 코드 5을 사용하세요.

모델을 언로드하고 메모리를 해제하려면 원본 텍스트의 코드 6을 사용하세요.

또는 Ollama 서버를 시작할 때 `OLLAMA_KEEP_ALIVE` 환경 변수를 설정하여 모든 모델이 메모리에 로드되는 시간을 변경할 수 있습니다. `OLLAMA_KEEP_ALIVE` 변수는 위에서 언급한 `keep_alive` 매개변수와 동일한 매개변수 유형을 사용할 수 있습니다. 환경 변수를 올바르게 설정하는 방법에 대한 [Ollama 서버 구성 방법](#how-do-i-configure-ollama-server) 섹션을 참조하세요.

`/api/generate`와 `/api/chat` API 엔드포인트의 `keep_alive` API 매개변수는 `OLLAMA_KEEP_ALIVE` 설정을 덮어씁니다.

## Ollama 서버가 요청을 대기하는 최대 수를 관리하는 방법은 무엇인가요?

서버에 너무 많은 요청이 전송되면 서버가 과부하 상태임을 나타내는 503 오류로 응답합니다. 대기할 수 있는 요청 수를 조정하려면 `OLLAMA_MAX_QUEUE`를 설정하세요.

## Ollama는 동시 요청을 어떻게 처리하나요?

Ollama는 두 가지 수준의 동시 처리를 지원합니다. 시스템에 사용 가능한 메모리가 충분하다면(CPU 추론 시 시스템 메모리 또는 GPU 추론을 위한 VRAM) 여러 모델을 동시에 로드할 수 있습니다. 특정 모델의 경우, 모델이 로드될 때 사용 가능한 메모리가 충분하면 병렬 요청 처리를 허용하도록 구성됩니다.

하나 이상의 모델이 이미 로드된 상태에서 새 모델 요청을 로드할 수 있는 사용 가능한 메모리가 부족한 경우, 모든 새로운 요청은 새 모델이 로드될 수 있을 때까지 대기하게 됩니다. 이전 모델이 유휴 상태가 되면, 하나 이상의 모델이 언로드되어 새 모델을 위한 공간을 만듭니다. 대기 중인 요청은 순서대로 처리됩니다. GPU 추론을 사용할 때는 새 모델이 VRAM에 완전히 맞아야 동시 모델 로드가 가능합니다.

특정 모델에 대한 병렬 요청 처리는 병렬 요청 수에 따라 컨텍스트 크기를 증가시킵니다. 예를 들어, 4개의 병렬 요청이 있는 2K 컨텍스트는 8K 컨텍스트와 추가 메모리 할당을 초래합니다.

다음 서버 설정을 사용하여 Ollama가 대부분의 플랫폼에서 동시 요청을 처리하는 방식을 조정할 수 있습니다:

- `OLLAMA_MAX_LOADED_MODELS` - 사용 가능한 메모리에 맞추어 동시에 로드할 수 있는 최대 모델 수입니다. 기본값은 GPU 수에 3을 곱한 값 또는 CPU 추론의 경우 3입니다.
- `OLLAMA_NUM_PARALLEL` - 각 모델이 동시에 처리할 수 있는 최대 병렬 요청 수입니다. 기본값은 사용 가능한 메모리에 따라 자동으로 4 또는 1로 선택됩니다.
- `OLLAMA_MAX_QUEUE` - 바쁠 때 Ollama가 추가 요청을 거부하기 전에 대기할 수 있는 최대 요청 수입니다. 기본값은 512입니다.

참고: 현재 Radeon GPU가 장착된 Windows에서는 ROCm v5.7의 VRAM 보고 제한으로 인해 기본적으로 최대 1개의 모델만 로드할 수 있습니다. ROCm v6.2가 출시되면 Windows Radeon도 위의 기본값을 따릅니다. Windows에서 Radeon에 대해 동시 모델 로드를 활성화할 수 있지만, GPU의 VRAM에 맞지 않는 모델을 로드하지 않도록 주의하세요.

## Ollama는 여러 GPU에서 모델을 어떻게 로드하나요?

새 모델을 로드할 때 Ollama는 모델에 필요한 VRAM을 현재 사용 가능한 VRAM과 비교합니다. 모델이 단일 GPU에 완전히 맞는다면, Ollama는 해당 GPU에 모델을 로드합니다. 이는 추론 중 PCI 버스를 통한 데이터 전송량을 줄여 성능을 최적화하는 데 일반적으로 가장 좋습니다. 모델이 단일 GPU에 완전히 맞지 않는 경우, 사용 가능한 모든 GPU에 분산됩니다.

## 플래시 주의 기능을 어떻게 활성화하나요?

플래시 주의 기능은 컨텍스트 크기가 커질 때 메모리 사용량을 크게 줄일 수 있는 대부분의 최신 모델의 기능입니다. 플래시 주의 기능을 활성화하려면 Ollama 서버를 시작할 때 `OLLAMA_FLASH_ATTENTION` 환경 변수를 `1`로 설정하세요.

## K/V 캐시의 양자화 유형을 어떻게 설정하나요?

플래시 주의 기능이 활성화된 경우 K/V 컨텍스트 캐시는 양자화하여 메모리 사용량을 크게 줄일 수 있습니다.

Ollama에서 양자화된 K/V 캐시를 사용하려면 다음 환경 변수를 설정할 수 있습니다:

- `OLLAMA_KV_CACHE_TYPE` - K/V 캐시의 양자화 유형입니다. 기본값은 `f16`입니다.

> 참고: 현재 이것은 전역 옵션으로, 모든 모델이 지정된 양자화 유형으로 실행됩니다.

현재 사용 가능한 K/V 캐시 양자화 유형은 다음과 같습니다:

- `f16` - 높은 정밀도와 메모리 사용량 (기본값).
- `q8_0` - 8비트 양자화로, `f16`의 메모리의 약 절반을 사용하며 정밀도가 아주 조금 손실되지만, 모델 품질에 눈에 띄는 영향은 없습니다 (f16을 사용하지 않을 경우 권장).
- `q4_0` - 4비트 양자화로, `f16`의 메모리의 약 1/4을 사용하며 정밀도가 소규모에서 중간 정도로 손실될 수 있습니다. 이는 더 높은 컨텍스트 크기에서 더 눈에 띄게 나타날 수 있습니다.

캐시 양자화가 모델의 응답 품질에 미치는 영향은 모델과 작업에 따라 다릅니다. GQA 수가 높은 모델(예: Qwen2)은 GQA 수가 낮은 모델보다 양자화로 인한 정밀도에 더 큰 영향을 받을 수 있습니다.

메모리 사용량과 품질 간의 최상의 균형을 찾기 위해 다양한 양자화 유형을 실험해 보아야 할 수도 있습니다.